
<!DOCTYPE html>
<html>
<head>
 <style>
    a:link {text-decoration: none; color:blue;} /*未访问的链接显示为蓝色，text-decoration:none将下划线隐藏*/
    a:visited {color:black;} /*用户已访问过的链接显示黑色*/
    a:hover {color:pink;}   /*鼠标放置在链接上时显示为粉色*/
    a:active {color:yellow;}  /* 链接被点击那一刻显示黄色 */

  /* 使背景图片容器充满整个屏幕 */
        .banner {
            width: 100%;
            height: 400px; /* 或根据需求调整高度 */
            background-image: url('https://upyun.hw.85do.com/uploads/20240828/5a06a23e0d72abbc260b36760e80a5f9.jpeg');
            background-size: cover;
            background-position: center;
            position: relative;
            display: flex;
            justify-content: center;  /* 水平居中 */
            align-items: center;      /* 垂直居中 */
            color: white;
            text-align: center; /* 让文字居中 */
        }

        .banner-msg h2, .banner-msg h3 {
            margin: 0;
        }

        .banner-msg h3 {
            margin-top: 5px;
        }
</style>
</head>
<body>
 <div style="display: block; margin: 0 auto; width: 100%; background: #F5F5DC;">
 <div style="display: block; margin: 0 auto; width: 80%; background: white;">
 <div style="display: block; margin: 0 auto; width: 70%; background: white;">

<!-- <p align="center";><img src="image/session.jpg"></p> -->
  <!-- 保留一张图片 -->
<div class="banner">
    <div class="banner-msg">
        <h2>IEEE Congress on Evolutionary Computation</h2>
        <h3>June 8 - 12, 2025, Hangzhou, China</h3>
    </div>
</div>
<p align="justify">
<br/>
<br/>
<p align="center";><h1 padding:20px; font-size:23px "Microsoft yahei""; align="center";>CEC'25 Competition:</h1></p>
<br/>
<br/>
<p align="center";><h1 padding:20px; font-size:28px "Microsoft yahei""; align="center";>Dynamic Multiobjective Optimisation</h1></p>
<br/>
<br/>
<p align="center";><h1 padding:20px; font-size:12px "Microsoft yahei""; align="center";>2025 IEEE Congress on Evolutionary Computation (CEC 2025)</h1></p>
<br/>
<br/>
<p align="center";><h1 padding:20px; font-size:12px "Microsoft yahei""; align="center";>Conference dates: June 8th to 12th 2025</h1></p>
<br/>
<br/>
<p align="center";><h1 padding:20px; font-size:12px "Microsoft yahei""; align="center";>Hangzhou, China</h1></p>
<br/>
<br/>
<p align="center";><h1 padding:20px; font-size:12px "Microsoft yahei""; align="center";>Organizers</h1></p>
<br/>
<br/>
<p align="center";><h1 padding:20px; font-size:12px "Microsoft yahei""; align="center";>Juan Zou</h1></p>
<p align="center";><h1 padding:20px; font-size:12px "Microsoft yahei""; align="center";>Xiangtan University, China</h1></p>
<br/>
<br/>
<p align="center";><h1 padding:20px; font-size:12px "Microsoft yahei""; align="center";>Shouyong Jiang</h1></p>
<p align="center";><h1 padding:20px; font-size:12px "Microsoft yahei""; align="center";>Xiangtan University, China</h1></p>
<br/>
<br/>
<p align="center";><h1 padding:20px; font-size:12px "Microsoft yahei""; align="center";>Zhanglu Hou</h1></p>
<p align="center";><h1 padding:20px; font-size:32px "Microsoft yahei""; align="center";>Xiangtan University, China</h1></p>
<br/>
<br/>
<p align="center";><h1 padding:20px; font-size:12px "Microsoft yahei""; align="center";>Xiaozhong Yu</h1></p>
<p align="center";><h1 padding:20px; font-size:12px "Microsoft yahei""; align="center";>Xiangtan University, China</h1></p>
<br/>
<br/>
<p align="center";><h1 padding:20px; font-size:12px "Microsoft yahei""; align="center";>Yaru Hu</h1></p>
<p align="center";><h1 padding:20px; font-size:32px "Microsoft yahei""; align="center";>Xiangtan University, China</h1></p>
<br/>

<p align="center";><h1 padding:20px; font-size:12px "Microsoft yahei""; align="center";>Shengxiang Yang</h1></p>
<p align="center";><h1 padding:20px; font-size:12px "Microsoft yahei""; align="center";>Xiangtan University, China</h1></p>
<br/>
<br/>
<b>Introduction</b>
The past decade has witnessed a growing amount of research interest in dynamic multiobjective optimisation, a challenging yet very important topic that deals with problems with multi-objective and time-varying properties. Due to the presence of dynamics, DMOPs are inherently more complex and challenging than static multiobjective problems, posing significant difficulties for evolutionary algorithms (EAs) in solving them. Generally speaking, DMOPs pose at least three main challenges. First, environmental changes are difficult to detect and, if undetected, can mislead the search process, as nondominated solutions found for the previous environment may no longer be valid in the current one. Second, diversity, the key driving force of population-based algorithms, is highly sensitive to dynamics. The dynamic nature of DMOPs, characterized by irregular changes, multimodality, and discrete Pareto optimal sets (PSs) or fronts (PFs), significantly complicates the optimisation process. Finally, the response time for environmental changes is often tight for algorithms. The time constraints on DMOPs require algorithms to strike a balance between diversity and convergence, enabling them to promptly handle environmental changes and closely track time-varying PSs or PFs. These challenges highlight the imperative to introduce more complex and comprehensive test problems, thereby fostering the development of innovative methodologies to tackle them.<br/><br/>
Benchmark problems are of great importance to algorithm analysis, which helps algorithm designers and practitioners to better understand the strengths and weaknesses of evolutionary algorithms. In dynamic multi-objective optimisation, there exist several widely used test suites, including FDA, dMOP and JY. However, these problem suites simplify the complexity of variations in real-world problems and only represent certain aspects of actual scenarios. For example, the FDA and dMOP functions have no detection difficulty for algorithms. Environmental changes involved in these problems can be easily detected with one re-evaluation of a random population member. Real-life environmental changes should not be so simple. It has also been recognised that most existing DMOPs are a direct modification of popular static test suites, e.g. ZDT and DTLZ. As a result, the DMOPs are more or less the same regarding their problem properties, and therefore are of limited use when a comprehensive algorithm analysis is pursued. Furthermore, another worrying characteristic of most existing DMOPs is that static problem properties overweigh too much dynamics. A problem property (e.g. strong variable dependency) that is challenging for static multiobjective optimisation may not be a good candidate for dynamic multiobjective optimisation. One reason for this is that a failure of algorithms for DMOPs is not due to the presence of dynamics, but rather the static property. It is therefore likely to get a misleading conclusion on the performance of algorithms when such DMOPs are used. Additionally, most benchmark designs are based on the assumption that the environments before and after the change are similar. However, in real-world scenarios, many DMOPs involve irregular environmental changes. In such cases, the search directions used by EAs for the current environment may not be suitable for the new environment, especially when the true PS of the new environment significantly deviates from, and in the worst case even points in the opposite direction to, that of the current environment. In a nutshell, a set of diverse and unbiased benchmark test problems for a systematic study of evolutionary algorithms are greatly needed in the area.<br/><br/>

<b>Competition Guideline</b>
In this competition, a total of 20 benchmark functions are introduced [1-4], covering representative types of DMOPs (continuous, and constrained) with diverse properties found in various real-world scenarios, such as irregular changes of PS or PF, time-dependent PF/PS geometries, disconnectivity, degeneration, detectability, and a changing number of decision variables and/or objective functions. Through suggesting a set of benchmark functions with a good representation of various real-world scenarios, we aim to promote the research on evolutionary dynamic multiobjective optimisation. All the benchmark functions have been implemented in MATLAB code.
<br/>
<br/>
The benchmarks used for competition are detailed in the following technical report (download here), which includes the necessary information to understand the problem, how the solutions are represented, and how the fitness function is evaluated. Please contact Dr Xiaozhong Yu. (xzyu@smail.xtu.edu.cn) if you encounter any problem.
<br/>
<br/>
There are two tracks of competition, briefly described as follows:<br/>
Track 1: Dynamic Unconstrained Multi-Objective Optimisation<br/>
Track 2: Dynamic Constrained Multi-Objective Optimisation<br/>
Please feel free to choose either one or multiple tracks for competition. However, you should make a separate submission for each track you have chosen.<br/><br/>
J. Zou, S. Jiang, Z. Hou, X. Yu, Y. Hu and S. Yang, " Benchmark Problems for CEC’2025 Competition on Dynamic Multiobjective Optimisation," technical Report., China, January, 2025.<br/><br/>

All the benchmark functions have been implemented in MATLAB code (download here). Source code for sampling on the PF is also provided (download here). Please note that, the sampled points of PF should be truncated properly to guarantee uniformity (the k-nearest neighbor method in SPEA2[5] could be used for this purpose). Your results of the competition can be submitted in the form of a brief technical report, which should be sent directly to Dr Xiaozhong Yu. Submissions in both forms will be considered as entries, therefore be ranked according to the competition evaluation criteria.<br/><br/>









<b>TEST SUITES</b>
<br/>
Single-objective and multi-objective continuous optimization have been intensively studied in the community of evolutionary optimization where many well-known test suites are available. As a preliminary attempt, we have designed two MTO test suites [8],[9] for single-objective and multi-objective continuous optimization tasks, respectively.<br/><br/>
The test suite for multi-task single-objective optimization (MTSOO) contains nine MTO complex problems, and ten 50-task MTO benchmark problems. Each of the complex MTO problem consists of two single-objective continuous optimization tasks, while each of the 50-task MTO problem contains 50 single-objective continuous optimization tasks, which bear certain commonality and complementarity in terms of the global optimum and the fitness landscape. These MTO problems possess different degrees of latent synergy between their involved component tasks.<br/><br/>
The test suite for multi-task multi-objective optimization (MTMOO) includes nine MTO complex problems, and ten 50-task MTO benchmark problems. Each of the complex MTO problem consists of two multi-objective continuous optimization tasks, while each of the 50-task MTO problem contains 50 multi-objective continuous optimization tasks, which bear certain commonality and complementarity in terms of the Pareto optimal solutions and the fitness landscape. The MTO problems feature different degrees of latent synergy between their involved two component tasks.<br/><br/>

All benchmark problems included in these two test suites are elaborated in technical reports [8],[9], respectively. Their associated code in Matlab is downloadable at 
<a href="https://www.hou-yq.com/WCCI-Competition.rar" target="_self" name="WCCI-Competition" title="WCCI-Competition">here</a>.<br/><br/>

<b>COMPETITION PROTOCOL</b>
<br/>
Potential participants in this competition may target at either or both of MTSOO and MTMOO while using all benchmark problems in the corresponding test suites as described above for performance evaluation.<br/><br/>
For MTSOO test suite:
<br/>
<br/>
<u>(1) Experimental settings</u><br/>
For each of 19 benchmark problems in this test suite, an algorithm is required to be executed for 30 runs where each run should employ different random seeds for the pseudo-random number generator(s) used in the algorithm. <b>Note:</b> It is prohibited to execute multiple 30 runs and deliberately pick up the best one.<br/><br/>
For all 2-task benchmark problems, the maximal number of function evaluations (maxFEs) used to terminate an algorithm in a run is set to 200,000, while the maxFEs is set to 5,000,000 for all 50-task benchmark problems. In the multitasking scenario, one function evaluation means calculation of the objective function value of any component task without distinguishing different tasks.<br/><br/>
<b>Note:</b> The parameter setting of an algorithm is required to be identical for each benchmark problem in this test suite, respectively. Participants are required to report the used parameter setting for each problem in the final submission to the competition. Please refer to “SUBMISSION GUIDELINE” for more details.<br/><br/>

<u>(2) Intermediate results required to be recorded</u><br/>
When an algorithm is executed to solve a specific benchmark problem in a run, the so far achieved best function error value (BFEV) w.r.t. each component task of this problem should be recorded when the current number of function evaluations reaches any of the predefined values which are set to k*maxFEs/Z, (k =1, …, Z; Z=100 for 2-task MTO problems and Z=1000 for 50-task MTO problems), in this competition. BFEV is calculated as the difference between the best objective function value achieved so far and the globally optimal objective function value known in advance. As a result, 100 BFEVs would be recorded for every 2-task benchmark problem, while 1000 BFEVs would be recorded for every 50-task benchmark problem, w.r.t. each component task in each run.
<br/>
<br/>
Intermediate results for each benchmark problem are required to be saved separately into nine “.txt” files named as "MTOSOO_P1.txt", …, "MTOSOO_P9.txt" for the nine MTO complex problems, and "MTOMSO_P1.txt", …, "MTOMSO_P10.txt" for the ten 50-task MTO benchmark problems.
<br/>
<br/>
1*maxFEs/Z, BFEV_{1,1}^1,…, BFEV_{1,1}^n,……, BFEV_{30,1}^1,…, BFEV_{30,1}^n<br/>
2*maxFEs/Z, BFEV_{1,2}^1,…, BFEV_{1,2}^n,……, BFEV_{30,2}^1,…, BFEV_{30,2}^n.<br/>
…<br/>
m*maxFEs/Z, BFEV_{1,m}^1,…, BFEV_{1,m}^n,……, BFEV_{30,m}^1,…, BFEV_{30,m}^n<br/>
<br/>

where BFEV_{j,k}^i (i = 1, …, n ;j = 1, …, 30; k = 1, …, m) stands for the BFEV w.r.t. the ith component task obtained in the jth run at the kth predefined number of function evaluations. Note: n=2 and m=100 for 2-task benchmark problems, while n=50 and m=1000 for 50-task benchmark problems.<br/><br/>
The first column stores the predefined numbers of function evaluations at which intermediate results are recorded. The subsequent columns store intermediate results for each of 30 runs with each run occupying n consecutive columns w.r.t. n component tasks, respectively. Note: The comma is used as a delimiter to separate any two numbers next to each other in a row. As an example, “.txt” files obtained by MFEA are provided as reference.
<br/>
<br/>
<u>(3) Overall ranking criterion</u><br/>
To derive the overall ranking for each algorithm participating in the competition, we will take into account of the performance of an algorithm on each component task in each benchmark problem under varying computational budgets from small to large. Specifically, we will treat each component task in each benchmark problem as one individual task, ending up with a total of 518 individual tasks. For each algorithm to be ranked, the median BFEV over 30 runs will be calculated at each checkpoint which corresponds to different computational budgets for each of 518 individual tasks. Based on these calculated data, the overall ranking criterion will be defined. To avoid deliberate calibration of the algorithm to cater for the overall ranking criterion, we will release the formulation of the overall ranking criterion after the competition submission deadline.
<br/>
<br/>
<b>For MTMOO test suite:</b><br/><br/>
<u>(1) Experimental settings</u><br/>

For each of 19 benchmark problems in this test suite, an algorithm is required to be executed for 30 runs where each run should employ different random seeds for the pseudo-random number generator(s) used in the algorithm. <b>Note:</b> It is prohibited to execute multiple 30 runs and deliberately pick up the best one.
<br/>
<br/>
For all 2-task benchmark problems, the maximal number of function evaluations (maxFEs) used to terminate an algorithm in a run is set to 200,000, while the maxFEs is set to 5,000,000 for all 50-task benchmark problems. In the multitasking scenario, one function evaluation means calculation of the values of multiple objective functions of any component task without distinguishing different tasks.<br/>
<b>Note</b>: The parameter setting of an algorithm is required to be identical for each benchmark problem in this test suite, respectively. Participants are required to report the used parameter setting for each problem in the final submission to the competition. Please refer to “SUBMISSION GUIDELINE” for more details.
<br/>
<br/>
<u>(2) Intermediate results required to be recorded</u><br/>
When an algorithm is executed to solve a specific benchmark problem in a run, the obtained inverted generational distance (IGD) value w.r.t. each component task of this problem should be recorded when the current number of function evaluations reaches any of the predefined values which are set to k*maxFEs/Z, (k =1, …, Z; Z=100 for 2-task MTO problems and Z=1000 for 50-task MTO problems), in this competition. IGD [10] is a commonly used performance metric in multi-objective optimization to evaluate the quality (convergence and diversity) of the currently obtained Pareto front by comparing it to the optimal Pareto front known in advance. As a result, 100 IGD values would be recorded for every 2-task benchmark problem, while 1000 IGD values would be recorded for every 50-task benchmark problem, w.r.t. each component task in each run.
<br/>
<br/>
Intermediate results for each benchmark problem are required to be saved into separate ".txt" files: "MTOMOO_P1.txt", …, "MTOMOO_P9.txt" for the nine MTO complex problems, and "MTOMMO_P1.txt", …, "MTOMMO_P10.txt" for the ten 50-task MTO benchmark problems. The data contained in each ".txt" file must conform to the following format:
<br/>
<br/>
1*maxFEs/Z, IGD_{1,1}^1,…, IGD_{1,1}^n,……, IGD_{30,1}^1,…, IGD_{30,1}^n<br/>
2*maxFEs/Z, IGD_{1,2}^1,…, IGD_{1,2}^n,……, IGD_{30,2}^1,…, IGD_{30,2}^n<br/>
…<br/>
m*maxFEs/Z, IGD_{1,m}^1,…, IGD_{1,m}^n,……, IGD_{30,m}^1,…, IGD_{30,m}^n<br/>
<br/>
where IGD_{j,k}^i (i = 1, …, n ;j = 1, …, 30; k = 1, …, m) stands for the IGD value w.r.t. the ith component task obtained in the jth run at the kth predefined number of function evaluations. Note: n=2 and m=100 for 2-task benchmark problems, while n=50 and m=1000 for 50-task benchmark problems. <br/><br/>
The first column stores the predefined numbers of function evaluations at which intermediate results are recorded. The subsequent columns store intermediate results for each of 30 runs with each run occupying n consecutive columns w.r.t. n component tasks, respectively. Note: The comma is used as a delimiter to separate any two numbers next to each other in a row. As an example, “.txt” files obtained by MFEA are provided as reference.
<br/>
<br/>
<u>(3) Overall ranking criterion</u><br/>
To derive the overall ranking for each algorithm participating in the competition, we will take into account of the performance of an algorithm on each component task in each benchmark problem under varying computational budgets from small to large. Specifically, we will treat each component task in each benchmark problem as one individual task, ending up with a total of 518 individual tasks. For each algorithm compared for ranking, the median IGD value over 30 runs will be calculated at each checkpoint corresponding to different computational budgets for each of 518 individual tasks. Based on these calculated data, the overall ranking criterion will be defined. To avoid deliberate calibration of the algorithm to cater for the overall ranking criterion, we will release the formulation of the overall ranking criterion after the competition submission deadline.
<br/>
<br/>

<b>SUBMISSION GUIDELINE</b><br/>
please archive the following files into a single .zip file and then send it to <font color="red">CompetitionMTO@gmail.com</font> before the competition submission deadline (1 June 2025):<br/><br/>
For participants in MTSOO: 19 ".txt" files (i.e., "MTOSOO_P1.txt", … , "MTOSOO_P10.txt"), "param_SO.txt" and "code.zip".<br/><br/>
For participants in MTMOO: 19 “.txt” files (i.e., "MTOMOO_P1.txt", … , "MTOMOO_P9.txt"), "param_MO.txt" and "code.zip".<br/><br/>
For participants in both MTSOO and MTMOO: 38 ".txt" files (i.e., the required ".txt" files for both MTSOO and MTMOO), "param_SO.txt", "param_MO.txt" and "code.zip".<br/><br/>
Here, "param_SO.txt" and "param_MO.txt" contain the parameter setting of the algorithm for MTSOO and MTMOO test suites, respectively. "code.zip" contains the source code of the algorithm which should allow the generation of reproducible results.<br/><br/>
If you would like to participate in the competition, please kindly inform us about your interest via email (CompetitionMTO@gmail.com) so that we can update you about any bug fixings and/or the extension of the deadline.
<br/>
<br/>
<b>COMPETITION ORGNIZERS:</b>
<br/>
<br/>
<b>Yaqing Hou</b><br/>
School of Computer Science and Technology, Dalian University of Technology, China<br/>
E-mail: houyq@dlut.edu.cn<br/>
Short Bio: Yaqing Hou received the Ph.D. degree in artificial intelligence from Interdisciplinary Graduate School, Nanyang Technological University, Singapore, in 2017. He was a Postdoctoral Research Fellow with Data Science and Artificial Intelligence Research Centre, Nanyang Technological University. He is currently an Associate Professor with the School of Computer Science and Technology, Dalian University of Technology, Dalian, China. His research interests include computational and artificial intelligence, multi-agent learning systems, evolutionary multi-tasking and transfer optimization. He is now an Associate Editor of the IEEE Transactions on Cognitive and Developmental Systems, Memetic Computing. He had co-organized multiple Special Issues on Memetic Computing journal and Applied Soft computing.
<br/>
<br/>

<b>Liang Feng</b><br/>
Chongqing University, College of Computer Science, China<br/>
E-mail: liangf@cqu.edu.cn<br/>
Short Bio:Liang Feng received his Ph.D degree from the School of Computer Engineering, Nanyang Technological University, Singapore, in 2014. He was a Postdoctoral Research Fellow at the Computational Intelligence Graduate Lab, Nanyang Technological University, Singapore. He is currently a Professor at the College of Computer Science, Chongqing University, China. His research interests include Computational and Artificial Intelligence, Memetic Computing, Big Data Optimization and Learning, as well as Transfer Learning. He has been honored with the 2019 IEEE Transactions on Evolutionary Computation Outstanding Paper Award, the 2023 IEEE TETCI Outstanding Paper Award, and the 2024 IEEE CIM Outstanding Paper Award. He is an Associate Editor of the IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE, IEEE CIM, and Memetic Computing. He is also the founding Chair of the IEEE CIS Intelligent Systems Applications Technical Committee Task Force on “Transfer Learning & Transfer Optimization” and the PC member of the IEEE Task Force on “Memetic Computing”. He had co-organized and chaired the Special Session on “Memetic Computing” and “Evolutionary Transfer Learning and Transfer Optimisation” held at IEEE CEC since 2016.
<br/>
<br/>
<b>Kai Qin</b><br/>
Department of Computer Science and Software Engineering<br/>
Swinburne University of Technology, Australia<br/>
E-mail: kqin@swin.edu.au<br/>
Website: http://www.alexkaiqin.org/<br/>
Short Bio:Kai Qin is a Professor at Swinburne University of Technology, Melbourne, Australia. Currently, he is the Director of Swinburne Intelligent Data Analytics Lab and the Deputy Director of Swinburne Space Technology and Industry Institute. Before joining Swinburne, he worked at Nanyang Technological University (Singapore), the University of Waterloo (Canada), INRIA Grenoble Rhône-Alpes (France), and RMIT University (Australia). His major research interests include machine learning, evolutionary computation, collaborative learning and optimization, computer vision, remote sensing, services computing, and edge computing. He was a recipient of the 2012 IEEE Transactions on Evolutionary Computation Outstanding Paper Award and the 2022 IEEE Transactions on Neural Networks and Learning Systems Outstanding Associate Editor. He is currently the Chair of the IEEE Computational Intelligence Society (CIS) Student Activities and Young Professionals Sub-committee, the Vice-Chair of the IEEE CIS Neural Networks Technical Committee, the Vice-Chair of the IEEE CIS Emergent Technologies Task Force on “Multitask Learning and Multitask Optimization”, the Vice-Chair of the IEEE CIS Neural Networks Task Force on “Deep Edge Intelligence, and the Chair of the IEEE CIS Neural Networks Task Force on “Deep Vision in Space”. He serves as the Associate Editor for several top-tier journals, e.g., IEEE TEVC, IEEE TNNLS, IEEE CIM, NNs, and SWEVO. He was the General Co-Chair of the 2022 IEEE International Joint Conference on Neural Networks (IJCNN 2022) held in Padua, Italy, and was the Chair of the IEEE CIS Neural Networks Technical Committee during the 2021-2022 term.
<br/>
<br/>
<b>Abhishek Gupta</b><br/>
Singapore Institute of Manufacturing Technology (SIMTech), Agency for Science, Technology and Research (A*STAR), Singapore<br/>
E-mail: abhishek_gupta@simtech.a-star.edu.sg<br/>
Short Bio: Abhishek Gupta received the Ph.D degree in Engineering Science from the University of Auckland, New Zealand, in 2014. Over the past 5 years, Dr. Gupta has been working in the area of Memetic Computation, with particular focus on developing novel theories and algorithms in the topics of evolutionary transfer and multitask optimization. His pioneering work on evolutionary multitasking, in particular, was bestowed the 2019 IEEE Transactions on Evolutionary Computation Outstanding Paper Award by the IEEE Computational Intelligence Society (CIS). He is Associate Editor of the IEEE Transactions on Emerging Topics in Computational Intelligence, and is also the founding Chair of the IEEE CIS Emergent Technology Technical Committee (ETTC) Task Force on Multitask Learning and Multitask Optimization. He is currently appointed as a Scientist in the Singapore Institute of Manufacturing Technology (SIMTech), Agency for Science, Technology and Research (A*STAR). He also jointly serves as an Adjunct Research Scientist in the Data Science and Artificial Intelligence Research Center, School of Computer Science and Engineering, Nanyang Technological University, Singapore.
<br/>
<br/>
<b>Yuan Yuan</b><br/>
Department of Computer Science and Engineering, Michigan State University, USA<br/>
E-mail: yyuan@msu.edu<br/>
Short Bio:Yuan Yuan is a Postdoctoral Fellow in the Department of Computer Science and Engineering, Michigan State University, USA. He received the PhD degree with the Department of Computer Science and Technology, Tsinghua University, China, in July 2015. From January 2014 to January 2015 he was a visiting PhD student with the Centre of Excellence for Research in Computational Intelligence and Applications, University of Birmingham, UK. He worked as a Research Fellow at the School of Computer Science and Engineering, Nangyang Technological University, Singapore, from October 2015 to November 2016. His current research interests include multi-objective optimization, genetic improvement, and evolutionary multitasking. Two of his conference papers were nominated for the best paper at the GECCO 2014 and GECCO 2015, respectively.
<br/>
<br/>
<b>Eric Scott</b><br/>
Department of Computer Science, George Mason University, USA<br/>
E-mail: escott8@gmu.edu<br/>
Short Bio: Eric Scott is a PhD candidate at George Mason University and a Senior Artificial Intelligence Engineer at MITRE Corporation in Northern Virginia.  His research focuses on heuristic optimization algorithms and their applications to simulation and modeling in a variety of fields.  He holds a double B.Sc. in Computer Science and Mathematics from Andrews University in Berrien Springs, Michigan, and a M.Sc. in Computer Science from George Mason University.
<br/>
<br/>
<b>Yew-Soon Ong</b><br/>
School of Computer Science and Engineering, Nanyang Technological University, Singapore<br/>
E-mail: asysong@ntu.edu.sg<br/>
Website: http://www.ntu.edu.sg/home/asysong/<br/>
Short Bio:Yew-Soon Ong is Professor and Chair of the School of Computer Science and Engineering, Nanyang Technological University, Singapore. He is Director of the A*Star SIMTECH-NTU Joint Lab on Complex Systems and Programme Principal Investigator of the Data Analytics & Complex System Programme in the Rolls-Royce@NTU Corporate Lab. He was Director of the Centre for Computational Intelligence or Computational Intelligence Laboratory from 2008-2015. He received his Bachelors and Masters degrees in Electrical and Electronics Engineering from Nanyang Technological University and subsequently his PhD from University of Southampton, UK. He is founding Editor-In-Chief of the IEEE Transactions on Emerging Topics in Computational Intelligence, founding Technical Editor-In-Chief of Memetic Computing Journal (Springer), Associate Editor of IEEE Computational Intelligence Magazine, IEEE Transactions on Evolutionary Computation, IEEE Transactions on Neural Network & Learning Systems, IEEE Transactions on Cybernetics, IEEE Transactions on Big Data, International Journal of Systems Science, Soft Computing Journal, and chief editor of Book Series on Studies in Adaptation, Learning, and Optimization as well as Proceedings in Adaptation, Learning, and Optimization He is also guest editors of IEEE Transactions on Evolutionary Computation, IEEE Trans SMC-B, Soft Computing Journal, Journal of Genetic Programming and Evolvable Machines, co-edited several books, including Multi-Objective Memetic Algorithms, Evolutionary Computation in Dynamic and Uncertain Environments, and a volume on Advances in Natural Computation published by Springer Verlag. He served as Chair of the IEEE Computational Intelligence Society Emergent Technology Technical Committee (ETTC) from 2011-2012, and has been founding chair of the Task Force on Memetic Computing in ETTC since 2006 as well as a member of IEEE CIS Evolutionary Computation Technical Committee from 2008 - 2010. He was also Chair of the IEEE Computational Intelligence Society Intelligent Systems Applications Technical Committee (ISATC) from 2013-2014. His current research interests include computational intelligence spanning memetic computing, evolutionary optimization using approximation/surrogate/meta-models, complex design optimization, intelligent agents in game, and Big Data Analytics. His research grants comprises of external funding from both national and international partners that include National Grid Office, A*STAR, Singapore Technologies Dynamics, Boeing Research & Development (USA), Rolls-Royce (UK) and Honda Research Institute Europe (Germany), National Research Foundation and MDAGAMBIT. His research work on Memetic Algorithm was featured by Thomson Scientific's Essential Science Indicators as one of the most cited emerging area of research in August 2007. Recently, he was selected as a 2015 Thomson Reuters Highly Cited Researcher and 2015 World's Most Influential Scientific Minds. He also received the 2015 IEEE Computational Intelligence Magazine Outstanding Paper Award and the 2012 IEEE Transactions on Evolutionary Computation Outstanding Paper Award for his work pertaining to Memetic Computation.
<br/>
<br/>
<b>REFERENCES:</b><br/>
[1] R. Caruana, “Multitask learning”, Machine Learning, 28(1): 41-75, 1997.<br/>
[2] K. Swersky, J. Snoek and R. P. Adams, “Multi-task Bayesian optimization”, Proceedings of the 26th International Conference on Neural Information Processing Systems (NIPS'13), pp. 2004-2012, Lake Tahoe, Nevada, USA, December 5-10, 2013.<br/>
[3] A. Gupta, Y. S. Ong and L. Feng, “Multifactorial evolution: Toward evolutionary multitasking”, IEEE Transactions on Evolutionary Computation, 20(3):343-357, 2016.<br/>
[4] Y. S. Ong and A. Gupta, “Evolutionary multitasking: A computer science view of cognitive multitasking”, Cognitive Computation,  8(2): 125-142, 2016.<br/>
[5] A. Gupta, Y. S. Ong, L. Feng and K. C. Tan, “Multi-objective multifactorial optimization in evolutionary multitasking”, accepted by IEEE Transactions on Cybernetics, 2016.<br/>
[6] K. K. Bali, A. Gupta, Y. S. Ong, and P. S. Tan. "Cognizant Multitasking in Multi-Objective Multifactorial Evolution: MO-MFEA-II." IEEE Transactions on Cybernetics, 2020.<br/>
[7] Gupta, A., Zhou, L., Ong, Y. S., Chen, Z., & Hou, Y. "Half a dozen real-world applications of evolutionary multitasking, and more." IEEE Computational Intelligence Magazine 17.2 (2022): 49-66.<br/>
[8] B. S. Da, Y. S. Ong, L. Feng, A. K. Qin, A. Gupta, Z. X. Zhu, C. K. Ting, K. Tang and X. Yao, “Evolutionary multitasking for single-objective continuous optimization: Benchmark problems, performance metrics and baseline results”, Technical Report, Nanyang Technological University, 2016.<br/>
[9] Y. Yuan, Y. S. Ong, L. Feng, A. K. Qin, A. Gupta, B. S. Da, Q. F. Zhang, K. C. Tan, Y. C. Jin and H. Ishibuchi, “Evolutionary multitasking for multi-objective continuous optimization: Benchmark problems, performance metrics and baseline results”, Technical Report, Nanyang Technological University, 2016.<br/>
[10] P. Czyzzak and A. Jaszkiewicz, “Pareto simulated annealing– a metaheuristic technique for multiple-objective combinatorial optimization”, Journal of Multi-Criteria Decision Analysis, 7:34-47, 1998.<br/>
<br/>
<br/>

</div>
</div>
</body>
</html>
